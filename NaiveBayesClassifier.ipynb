{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713eff8f-09c9-4b5f-b96c-3ed2ef8de1fa",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier implementation for Kaggle competition 1\n",
    "## Milestone 1: Beat coinflip prediction model and logistic regression prediction model\n",
    "\n",
    "### By Emiliano Aviles and Cassandre Hamel\n",
    "\n",
    "## I) Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a70a7f-99be-4cb3-b135-9080625146bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b72d9b5-235e-4d17-b755-d7ff8f488cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('data_train.npy', allow_pickle=True)\n",
    "\n",
    "data_test = np.load('data_test.npy', allow_pickle=True)\n",
    "\n",
    "labels_train = np.loadtxt('label_train.csv', delimiter=',', skiprows=1, usecols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb060ef-c9ba-42ca-b398-c901d3fcd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "split_ratio = 0.8\n",
    "n_samples = data_train.shape[0]\n",
    "n_train = int(n_samples * split_ratio)\n",
    "\n",
    "# Shuffle the dataset (important to ensure random splitting)\n",
    "shuffled_indices = np.random.permutation(n_samples)\n",
    "train_indices = shuffled_indices[:n_train]\n",
    "val_indices = shuffled_indices[n_train:]\n",
    "\n",
    "# Split data and labels into training and validation sets\n",
    "X_train, X_val = data_train[train_indices], data_train[val_indices]\n",
    "y_train, y_val = labels_train[train_indices], labels_train[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2af26c-1c82-4015-834b-58bdb20c095f",
   "metadata": {},
   "source": [
    "## II) Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8c6b05-7c02-4044-8ae3-5b37afc5fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_log_prior_ = None  # Log prior for each class\n",
    "        self.feature_log_prob_ = None  # Log probability of each feature given the class\n",
    "        self.classes_ = None  # The unique classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Get unique class labels and their counts\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Calculate log prior probabilities for each class\n",
    "        self.class_log_prior_ = np.log(class_counts / y.shape[0])\n",
    "        \n",
    "        # Calculate the number of features (words)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Initialize an array to count the occurrences of each feature for each class\n",
    "        feature_count = np.zeros((len(self.classes_), n_features))\n",
    "        \n",
    "        # Count occurrences of each feature for each class (with Laplace smoothing)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            feature_count[i, :] = X[y == c].sum(axis=0) + 1  # Add 1 for Laplace smoothing\n",
    "        \n",
    "        # Calculate the log probabilities of each feature given the class\n",
    "        feature_totals = feature_count.sum(axis=1, keepdims=True)\n",
    "        self.feature_log_prob_ = np.log(feature_count / feature_totals)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Compute the log probabilities for each class\n",
    "        log_probs = []\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            log_prob_c = self.class_log_prior_[i] + np.dot(X, self.feature_log_prob_[i].T)\n",
    "            log_probs.append(log_prob_c)\n",
    "        \n",
    "        log_probs = np.array(log_probs).T\n",
    "        \n",
    "        # Return the class with the highest log probability\n",
    "        return self.classes_[np.argmax(log_probs, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2a087-674a-45bd-8704-847ea9164661",
   "metadata": {},
   "source": [
    "## III) Model fitting and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5af129-bfe7-40a9-9768-eab72fdba0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Naive Bayes classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c9b815-9537-4e3b-b159-932dc34a9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7628647214854112\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "y_val_pred = nb_classifier.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_val_pred == y_val)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cb36e6-6739-42a2-8ed4-2affb2a6ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.7085080790374689\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate precision, recall, and F1 score\n",
    "def f1_score_macro(y_true, y_pred):\n",
    "    unique_classes = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))  # True Positives\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))  # False Positives\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))  # False Negatives\n",
    "        \n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Macro F1 score is the average of the F1 scores for each class\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score_macro(y_val, y_val_pred)\n",
    "print(f\"Macro F1 Score: {macro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51bd6209-d57f-4daf-8a30-f57d83149ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nb_classifier.predict(data_test)\n",
    "\n",
    "IDs = np.array(range(len(y_test_pred)))\n",
    "\n",
    "output = np.hstack((IDs.reshape(len(IDs), 1), y_test_pred.reshape(len(y_test_pred), 1)))\n",
    "\n",
    "# Save the predicted labels for the test set (to submit or evaluate externally)\n",
    "np.savetxt('test_predictions.csv', output, delimiter=',', fmt='%d', header='ID,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1421e9a-940d-4cca-994e-7608c947d160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
